---
title: "평가 루브릭 설계"
description: "학습자 과제 평가를 위해 모델/에이전트/RAG 결과를 해석하고 점수를 매기는 기준을 정의합니다."
icon: "scale-balanced"
---

## 평가 관점 정리
- **정확도**: 질문 의도에 맞는 답변과 핵심 근거를 포함했는지 확인합니다.
- **출처/인용**: RAG 사용 시 citation이 포함되었는지, 근거 문서가 적절한지 체크합니다.
- **비용·지연시간**: 동일 질문 기준으로 latency와 token, 비용을 비교해 효율성을 점수에 반영합니다.

---

## 루브릭 예시
| 항목 | 가중치 | 체크 포인트 |
| --- | --- | --- |
| 내용 정확성 | 40% | 질문 요구사항을 모두 충족했는가 |
| 인용 및 근거 | 30% | citation 포함 여부, 근거 문구의 적합성 |
| 표현/톤 | 10% | 시스템 프롬프트에 따른 스타일 준수 여부 |
| 비용/지연시간 | 20% | latency·token·cost가 제시된 목표 범위 내인지 |

---

## 평가 실행 흐름
1. **질문 세트 확정**: 쉬운/어려운 질문을 섞어 5~10개로 구성합니다.
2. **비교 버전 수집**: 기본 모델, 프롬프트 변형, RAG on/off, 에이전트 on/off 조합으로 최소 2~3개 결과를 제출받습니다.
3. **루브릭 채점**: 표의 가중치에 맞춰 점수를 매기고, 개선 필요 항목을 구체적으로 기록합니다.
4. **피드백 공유**: latency나 비용이 높은 경우 설정값(temperature, top-k 등)과 연결해 개선 방향을 제안합니다.

---

## 실습 후 회고 체크리스트
- [ ] 정답/오답 예시와 함께 점수를 공개했나요?
- [ ] 비용/지연시간을 기준으로 한 개선 팁을 제공했나요?
- [ ] citation 누락 사례를 모아 RAG 설정(문서 분할, threshold)을 조정했나요?
